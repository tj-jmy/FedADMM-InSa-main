import copy
import torch
import numpy as np
from torch import nn
from torch.utils.data import DataLoader
from utils.dataset import DatasetSplit
from utils.model import init_model


class Clients(object):
    """Clients' local update processes."""

    def __init__(self, cfg, server_model, dataset, alg):
        # åˆå§‹åŒ–å‡½æ•°ï¼Œç”¨äºè®¾ç½®å®¢æˆ·ç«¯çš„é…ç½®å’Œå‚æ•°
        self.cfg = cfg  # scenario config
        self.device = cfg.device  # gpu or cpu
        self.datasets = dataset  # clients' datasets
        # initialize parameters# è°ƒç”¨å†…éƒ¨æ–¹æ³•åˆå§‹åŒ–å‚æ•°ï¼ŒåŒ…æ‹¬æ‹‰æ ¼æœ—æ—¥ä¹˜å­å’Œæƒ©ç½šå‚æ•°
        self.alg = alg  # algorithm name
        self._init_params(cfg, server_model)
        self.alpha = self.cfg.alpha  # æ¯ä¸ªå®¢æˆ·ç«¯çš„æƒé‡


    def local_update(self, model_server, selected_clients, k):
        # model_serverï¼šä¼ å…¥çš„æœåŠ¡å™¨æ¨¡å‹ï¼Œç”¨äºåˆå§‹åŒ–å®¢æˆ·ç«¯æ¨¡å‹ï¼Œä½¿æ‰€æœ‰å®¢æˆ·ç«¯ä»ç›¸åŒçš„å‚æ•°å¼€å§‹æœ¬åœ°æ›´æ–°ã€‚
        # selected_clientsï¼šä¸€ä¸ªåˆ—è¡¨æˆ–é›†åˆï¼ŒåŒ…å«åœ¨å½“å‰è½®æ¬¡ä¸­é€‰ä¸­çš„å®¢æˆ·ç«¯ç´¢å¼•ã€‚ä»…åœ¨æ­¤åˆ—è¡¨ä¸­çš„å®¢æˆ·ç«¯ä¼šæ‰§è¡Œæœ¬åœ°æ›´æ–°ã€‚
        # kï¼šå½“å‰çš„è®­ç»ƒè½®æ¬¡ç¼–å·ï¼Œé€šå¸¸ç”¨äºæ§åˆ¶ç‰¹å®šè½®æ¬¡çš„æ“ä½œï¼Œæˆ–è®°å½•å®¢æˆ·ç«¯çš„è®­ç»ƒæ­¥æ•°ã€‚
        """Clients' local update processes."""

        # å°†æœåŠ¡å™¨æ¨¡å‹å’Œå½“å‰è½®æ¬¡ç¼–å·kèµ‹ç»™å®ä¾‹å˜é‡
        self.model_server = model_server
        self.k = k

        # éå†æ‰€æœ‰å®¢æˆ·ç«¯
        for self.i in range(self.cfg.m):
            # å¦‚æœå®¢æˆ·ç«¯iåœ¨æœ¬è½®æ¬¡ä¸­è¢«é€‰ä¸­ï¼Œåˆ™æ‰§è¡Œæœ¬åœ°æ›´æ–°
            if self.i in selected_clients:
                # Load client i's local settings
                # æ·±æ‹·è´æœåŠ¡å™¨æ¨¡å‹å¹¶å°†å…¶è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
                self.model_i = copy.deepcopy(self.model_server).train()
                # åŠ è½½å®¢æˆ·ç«¯içš„æ•°æ®é›†åˆ‡ç‰‡
                self.dataset_i = DatasetSplit(self.datasets, self.i)
                # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨å’Œä¼˜åŒ–å™¨
                data_loader = init_data_loader(self.cfg, self.dataset_i)
                optimizer = init_optimizer(self.cfg, self.model_i, k)
                # è®¡æ•°å·²å®Œæˆçš„SGDæ­¥æ•°å’Œè®­ç»ƒè½®æ¬¡

                # count_sï¼šç”¨äºç»Ÿè®¡å·²å®Œæˆçš„SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰æ­¥æ•°ã€‚æ¯å¤„ç†ä¸€ä¸ªæ•°æ®æ‰¹æ¬¡ï¼Œcount_så¢åŠ 1ï¼Œè¡¨ç¤ºå®Œæˆäº†ä¸€æ¬¡SGDæ›´æ–°ã€‚
                # count_eï¼šç”¨äºç»Ÿè®¡å·²å®Œæˆçš„è®­ç»ƒè½®æ¬¡ï¼ˆepochsï¼‰ã€‚åœ¨æ¯è½®æ¬¡ç»“æŸæ—¶ï¼Œcount_eå¢åŠ 1ï¼Œè¿™æ ·å¯ä»¥è®°å½•å®¢æˆ·ç«¯æœ¬åœ°æ•°æ®å·²å®Œæ•´éå†çš„æ¬¡æ•°ã€‚
                count_s, count_e = 0, 0  # finished SGD steps or epochs
                # Start local training å¼€å§‹æœ¬åœ°è®­ç»ƒè¿‡ç¨‹
                train_flag = True  # flag for keep local training
                while train_flag:  # update client i's model parameters u_i
                    # check the criterion after each epoch
                    if self._stop_check(count_e):
                        break
                    # éå†æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼Œæ‰§è¡ŒSGD
                    for data, labels in data_loader:  # iterate over batches
                        # å°†æ•°æ®å’Œæ ‡ç­¾ä¼ å…¥è®¾å¤‡ï¼ˆå¦‚GPUï¼‰
                        self.data, self.labels = data.to(
                            self.device), labels.to(self.device)
                        # æ‰§è¡Œä¸€æ¬¡æœ¬åœ°æ¢¯åº¦ä¸‹é™ï¼ˆFGDæˆ–SGDï¼‰
                        # perform single FGD/SGD step
                        self._local_step(self.cfg)
                        # æ›´æ–°æœ¬åœ°æ¨¡å‹å‚æ•°
                        optimizer.step()  # update client's model parameters u
                        count_s += 1
                    # å¢åŠ å·²å®Œæˆçš„è®­ç»ƒè½®æ¬¡è®¡æ•°
                    count_e += 1
                # Personalized actions of different algorithms.
                self._local_step_personalize(self.cfg)
                # å°†å®¢æˆ·ç«¯içš„æ¨¡å‹å‚æ•°ä¿å­˜åˆ°æ¨¡å‹å­—å…¸ä¸­
                self.models[self.i] = self.model_i.state_dict()
                self.steps[self.i][self.k] = count_e
                # è®°å½•å®¢æˆ·ç«¯içš„è®­ç»ƒè½®æ¬¡å’Œæ­¥æ•°
                # record local FGD/SGD steps
                self.res_dict["steps"] = self.steps
                # self.send_back é€šå¸¸æŒ‡çš„æ˜¯å®¢æˆ·ç«¯æ›´æ–°å®Œæˆåï¼Œå°†æ¨¡å‹æ›´æ–°çš„ç»“æœè¿”å›ç»™æœåŠ¡å™¨ç«¯è¿›è¡Œèšåˆã€‚åœ¨è”é‚¦å­¦ä¹ æˆ–åˆ†å¸ƒå¼ä¼˜åŒ–æ¡†æ¶ä¸­ï¼Œsend_backåŒ…å«å®¢æˆ·ç«¯çš„æ¨¡å‹å‚æ•°æˆ–å¯¹å¶å˜é‡æ›´æ–°ï¼Œä»¥ä¾¿æœåŠ¡å™¨è¿›è¡Œå…¨å±€æ›´æ–°ã€‚
                #
                # å…·ä½“æ¥è¯´ï¼Œsend_backçš„æ•°æ®å¯èƒ½åŒ…æ‹¬ï¼š
                #
                # æœ¬åœ°æ¨¡å‹å‚æ•°ï¼šç”¨äºæœåŠ¡å™¨çš„å…¨å±€æ¨¡å‹èšåˆã€‚
                # è®­ç»ƒæ­¥æ•°æˆ–çŠ¶æ€ï¼šå¸®åŠ©æœåŠ¡å™¨è·Ÿè¸ªæ¯ä¸ªå®¢æˆ·ç«¯çš„è®­ç»ƒè¿›å±•ã€‚
        return self.send_back

    def _stop_check(self, count):
        """Determine whether to stop client's local training."""
        stop_training = False
        # Check the maximum number. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è®­ç»ƒè½®æ¬¡ self.cfg.E
        if count >= self.cfg.E:
            stop_training = True  # è¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼Œè®¾ç½®åœæ­¢æ ‡è®°ä¸º True
        # Check the admm inexactness criterion.
        elif self.alg in ["admm_insa", "admm_in"]:  # æ£€æŸ¥ ADMM çš„ä¸ç²¾ç¡®å‡†åˆ™ï¼Œä»…åœ¨ç‰¹å®šç®—æ³•ä¸‹å¯ç”¨
            if self._inexact_check(count):  # å¦‚æœç¬¦åˆä¸ç²¾ç¡®æ¡ä»¶
                stop_training = True  # stop training # è®¾ç½®åœæ­¢æ ‡è®°ä¸º True
        return stop_training  # è¿”å›æœ€ç»ˆçš„åœæ­¢æ ‡è®°

    def _local_step(self, cfg):
        """Client i performs single FGD/SGD step.""""""å®¢æˆ·ç«¯ i æ‰§è¡Œä¸€æ¬¡ FGD/SGD æ›´æ–°ã€‚"""
        # å¦‚æœé€‰æ‹©äº† FedAvg ç®—æ³•ï¼Œåˆ™ä½¿ç”¨æ ‡å‡†çš„æ¢¯åº¦è®¡ç®—
        if self.alg in ["fedavg"]:
            self._grad(self.model_i, self.data, self.labels)
        # å¦‚æœé€‰æ‹©äº† ADMM ç±»ç®—æ³•ï¼ˆåŒ…æ‹¬ Vanilla ADMM å’Œè‡ªé€‚åº” ADMMï¼‰ï¼Œè¿›è¡Œ ADMM ç‰¹å®šçš„æ›´æ–°
        elif self.alg in ["admm", "admm_insa", "admm_in"]:
            # å¯¹äºè‡ªé€‚åº” ADMM ç®—æ³•ï¼ˆadmm_insa æˆ– admm_inï¼‰ï¼Œä¸”æ‰¹é‡å¤§å°ä¸º 0ï¼ˆå³éæ‰¹é‡å¤„ç†ï¼‰ï¼Œè·³è¿‡æ¢¯åº¦è®¡ç®—
            # åœ¨ admm_insa å’Œ admm_in ç®—æ³•ä¸­ï¼Œå½“æ‰¹é‡å¤§å° (bs) ä¸º 0 ä¸”åœ¨ä¸ç²¾ç¡®æ£€æŸ¥ä¸­å·²è®¡ç®—è¿‡æ¢¯åº¦æ—¶è·³è¿‡æ¢¯åº¦è®¡ç®—ï¼Œ
            # ä¸»è¦æ˜¯ä¸ºäº†é¿å…é‡å¤è®¡ç®—æ¢¯åº¦ï¼Œä»è€Œå‡å°‘è®¡ç®—è´Ÿæ‹…å’Œé€šä¿¡æˆæœ¬ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæ¢¯åº¦å·²åœ¨ _inexact_check ä¸­é¢„å…ˆè®¡ç®—å¹¶ç”¨äºåˆ¤æ–­æ˜¯å¦æ»¡è¶³ä¸ç²¾ç¡®å‡†åˆ™ï¼Œ
            # å› æ­¤ä¸éœ€è¦åœ¨æœ¬åœ°æ›´æ–°ä¸­é‡å¤è¿›è¡Œã€‚è¿™ä¸€ç­–ç•¥ä¼˜åŒ–äº†èµ„æºä½¿ç”¨ï¼Œå°¤å…¶åœ¨èµ„æºå—é™çš„åˆ†å¸ƒå¼ç¯å¢ƒä¸­æé«˜äº†æ•ˆç‡ã€‚
            if self.alg in ["admm_insa", "admm_in"] and self.cfg.bs == 0:
                pass  # grad calculated during the inexactness check, see self._inexact_e()# å¦‚æœå·²åœ¨ä¸ç²¾ç¡®æ£€æŸ¥ä¸­è®¡ç®—æ¢¯åº¦ï¼Œåˆ™æ­¤å¤„è·³è¿‡
            else:  # å¦åˆ™æ‰§è¡Œ ADMM çš„ u æ›´æ–°æ­¥éª¤
                self._admm_u(self.model_i, self.data, self.labels)
        else:  # å¦‚æœç®—æ³•æ— æ•ˆï¼ŒæŠ›å‡ºé”™è¯¯
            raise ValueError(f"Invalid algorithm.")

    def _local_step_personalize(self, cfg):
        """Personalized actions of different algorithms."""
        # ADMM related local update process.# é’ˆå¯¹ ADMM ç›¸å…³ç®—æ³•çš„ä¸ªæ€§åŒ–æ›´æ–°
        if self.alg in ["admm_insa", "admm_in", "admm"]:
            beta_i_k = self.beta[self.i][self.k]  # è·å–å½“å‰å®¢æˆ·ç«¯å’Œè½®æ¬¡çš„æƒ©ç½šå‚æ•° beta
            # update dual variables lambda  # æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜å­ lambda
            self._update_lambda(beta_i_k)
            if self.alg == "admm_insa":  # udpate penlaty parameter beta å¦‚æœæ˜¯ admm_insaï¼ˆè‡ªé€‚åº” ADMMï¼‰ï¼Œæ›´æ–°æƒ©ç½šå‚æ•° beta
                self._adp_beta(beta_i_k)  # è‡ªé€‚åº”è°ƒæ•´ beta
            # åœ¨ admm æ¨¡å¼ä¸‹çš„æƒ©ç½šå‚æ•°æ›´æ–°é€šå¸¸ä¸ä¼šè¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ï¼Œå› æ­¤ beta å‚æ•°ä¿æŒæ’å®šã€‚
            # å…·ä½“æ¥è¯´ï¼Œadmm ç›´æ¥ä½¿ç”¨å›ºå®šçš„æƒ©ç½šå‚æ•° beta_i_kï¼Œå¹¶åœ¨æ¯è½®æ›´æ–°ä¸­ä¿æŒä¸å˜ï¼Œä»¥ç¡®ä¿æ¯ä¸ªå®¢æˆ·ç«¯éµå¾ªç›¸åŒçš„æƒ©ç½šæ ‡å‡†ã€‚
            # record params to send back to the server  # è®°å½•æ›´æ–°åçš„ beta å‚æ•°ï¼Œä»¥ä¾¿ä¼ å›æœåŠ¡å™¨
            self.send_back["beta"] = self.beta[:, self.k]  # beta_i^k use new
            self.res_dict["beta"] = self.beta  # record beta to plot

    def _update_lambda(self, beta):
        """Update the dual variable lambda."""
        # è·å–å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨æ¨¡å‹çš„å‚æ•°çŠ¶æ€å­—å…¸
        u_state = self.model_i.state_dict()
        z_state = self.model_server.state_dict()
        # éå†æ¯ä¸ªæ¨¡å‹å‚æ•°ï¼ˆä¸åŒ…æ‹¬ BN ç»Ÿè®¡é‡ï¼‰
        for name, _ in self.model_i.named_parameters():  # without BN statistics
            # æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼šlambda = lambda - beta * (u - z)
            # self.lamda[self.i][name] -= beta * (u_state[name] - z_state[name])
            # æ ¹æ®æ–‡ç« 1çš„æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ›´æ–°å…¬å¼ï¼Œæ›´æ–°é€»è¾‘åº”æ”¹ä¸ºåŠ æ³•è€Œéå‡æ³•ã€‚
            self.lamda[self.i][name] += beta * (u_state[name] - z_state[name])

    def _adp_beta(self, beta):
        """Update beta based on the adaptive penalty scheme."""
        # è·å–å½“å‰è½®æ¬¡å’Œä¸Šä¸€è½®æ¬¡çš„æ¨¡å‹
        u_kplus1, z_k = self.model_i, self.model_server  # å½“å‰æ¨¡å‹ï¼ˆæœ¬è½®ï¼‰
        u_k = state2model(self.cfg, self.models[self.i])  # ä¸Šä¸€è½®æ¬¡çš„å®¢æˆ·ç«¯æ¨¡å‹
        # åŸå§‹æ®‹å·®ï¼Œç”¨äºåº¦é‡æœ¬åœ°æ¨¡å‹å’Œä¸Šæ¬¡æ›´æ–°é—´çš„å·®å¼‚ï¼Œè°ƒæ•´çº¦æŸå¼ºåº¦ã€‚
        primal_residual = beta * l2_norm(u_kplus1, u_k)
        dual_residual = l2_norm(u_kplus1, z_k)  # å¯¹å¶æ®‹å·®ï¼Œåº¦é‡æœ¬åœ°æ¨¡å‹å’Œå…¨å±€æ¨¡å‹é—´å·®å¼‚ã€‚
        # è‡ªé€‚åº”è°ƒæ•´ beta
        # å¢åŠ  betaï¼ˆå¢å¼ºçº¦æŸï¼‰ï¼šå½“åŸå§‹æ®‹å·®ç›¸å¯¹è¾ƒå°æ—¶ï¼Œè¯´æ˜å®¢æˆ·ç«¯æ¨¡å‹ä¸ä¸Šä¸€è½®æ¬¡çš„æ›´æ–°å·®å¼‚è¾ƒå°ï¼Œä½†ä¸å…¨å±€æ¨¡å‹çš„å·®å¼‚ï¼ˆå¯¹å¶æ®‹å·®ï¼‰è¾ƒå¤§ã€‚
        # æ­¤æ—¶éœ€è¦å¢å¼ºä¸€è‡´æ€§çº¦æŸï¼Œä½¿æœ¬åœ°æ¨¡å‹æ›´å¿«é€Ÿåœ°é€¼è¿‘å…¨å±€æ¨¡å‹ã€‚
        #
        # å‡å°‘ betaï¼ˆæ”¾å®½çº¦æŸï¼‰ï¼šå½“å¯¹å¶æ®‹å·®ç›¸å¯¹è¾ƒå°æ—¶ï¼Œè¯´æ˜æœ¬åœ°æ¨¡å‹ä¸å…¨å±€æ¨¡å‹å·²ç»æ¥è¿‘ï¼Œä½†æœ¬åœ°æ›´æ–°çš„æ­¥å¹…è¾ƒå¤§ï¼ˆåŸå§‹æ®‹å·®è¾ƒå¤§ï¼‰ã€‚
        # æ­¤æ—¶é™ä½ betaï¼Œå…è®¸æœ¬åœ°æ¨¡å‹æ‹¥æœ‰æ›´å¤§çš„æ›´æ–°çµæ´»æ€§ï¼Œæœ‰åˆ©äºé€‚åº”æœ¬åœ°æ•°æ®çš„ç‰¹æ€§ã€‚
        if self.cfg.mu * primal_residual < dual_residual:
            beta *= self.cfg.tau  # å¢åŠ  beta
        elif self.cfg.mu * dual_residual < primal_residual:
            beta /= self.cfg.tau  # å‡å°‘ beta
        self.beta[self.i][self.k + 1:] = beta  # update beta

    def _inexact_check(self, count):
        """Check the inexactness criterion."""
        # è®¡ç®—è‡ªé€‚åº”æƒ©ç½šå‚æ•° beta çš„è°ƒæ•´å€¼
        beta_tilde = self.beta[self.i][self.k] / self.cfg.c_i
        # è®¡ç®—ä¸ç²¾ç¡®å‡†åˆ™çš„å› å­ sigmaï¼Œç”¨äºè¡¡é‡å½“å‰æ¨¡å‹å˜åŒ–
        sigma = 0.999 * np.sqrt(2) / (np.sqrt(2) + np.sqrt(beta_tilde))
        # first time or using SGD, when e_u remains unchanged, calculate once and reuse
        # åˆå§‹æ£€æŸ¥ï¼ˆcount ä¸º 0ï¼‰ï¼Œæˆ–ä½¿ç”¨ SGD æ—¶è®¡ç®—åˆå§‹çš„ e_u å¹¶ç¼“å­˜ä»¥é‡å¤ä½¿ç”¨
        if count == 0:
            # use z^k instead of u_i^k# ä½¿ç”¨å…¨å±€æ¨¡å‹å‚æ•° z^k
            self.e_u = self._inexact_e(self.model_server)
        # è®¡ç®—æ–°çš„ä¸ç²¾ç¡®åº¦ e_u
        e_u_new = self._inexact_e(self.model_i)
        # å¦‚æœæ–°çš„ä¸ç²¾ç¡®åº¦ e_u_new å°äº sigma * self.e_uï¼Œåˆ™æ»¡è¶³ä¸ç²¾ç¡®å‡†åˆ™ï¼Œè¿”å› Trueï¼Œå¦åˆ™è¿”å› False
        return True if e_u_new <= sigma * self.e_u else False

    def _inexact_e(self, model: nn.Module):
        """Calculate the l2 norm of the inexactness residual e(u)."""

        data_loader_tmp = DataLoader(self.dataset_i, batch_size=500)
        model.eval()  # if using .train(), normalization layers cause problems
        model.zero_grad()
        for batch_idx, (data, labels) in enumerate(data_loader_tmp):
            data, labels = data.to(self.device), labels.to(self.device)
            pred = model(data)
            loss = model.loss(pred, labels)  # default reduction == "mean"
            loss.backward()  # accumulate grads
        u_state = model.state_dict()  # u_i
        z_state = self.model_server.state_dict()  # z
        for name, param in model.named_parameters():  # without BN statistics
            param.grad /= batch_idx + 1  # fix accumulated grads
            param.grad -= self.lamda[self.i][name]
            param.grad += self.beta[self.i][self.k] * \
                (u_state[name] - z_state[name])
        res = 0
        for param in model.parameters():  # without BN statistics
            res += torch.linalg.norm(param.grad).square()
        return res.sqrt().item()

    def _grad(self, model, data, labels):
        """Calculate the gradients of the local update."""
        # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼å¹¶æ¸…ç©ºæ¢¯åº¦
        model.train()
        model.zero_grad()
        # å‰å‘ä¼ æ’­ä»¥è·å¾—é¢„æµ‹
        pred = model(data)
        # è®¡ç®—æŸå¤±
        # default reduction == "mean"# é»˜è®¤ä½¿ç”¨ "mean" ä½œä¸ºæŸå¤±çš„å½’çº¦æ–¹å¼
        loss = model.loss(pred, labels)
        loss.backward()  # compute the gradients of f_i(u_i)# åå‘ä¼ æ’­è®¡ç®—æŸå¤±ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦

    def _admm_u(self, model, data, labels):
        """è®¡ç®— ADMM çš„ u å­é—®é¢˜çš„æ¢¯åº¦ã€‚"""
        """Calculate the gradients of the ADMM u-subproblem."""
        # è®¡ç®—æœ¬åœ°æ¨¡å‹çš„æ¢¯åº¦ï¼Œä½¿ç”¨æ•°æ®å’Œæ ‡ç­¾
        self._grad(model, data, labels)  # get gradients
        # è·å–å½“å‰å®¢æˆ·ç«¯æ¨¡å‹å’ŒæœåŠ¡å™¨å…¨å±€æ¨¡å‹çš„å‚æ•°çŠ¶æ€
        u_state = model.state_dict()  # u_i# æœ¬åœ°å®¢æˆ·ç«¯æ¨¡å‹ u_i
        z_state = self.model_server.state_dict()  # z# å…¨å±€æ¨¡å‹ z
        # æ›´æ–°æ¢¯åº¦ä»¥æ»¡è¶³ ADMM çš„ u å­é—®é¢˜
        for name, param in model.named_parameters():  # without BN statistics# å¿½ç•¥ BN ç»Ÿè®¡é‡
            # param.grad -= self.lamda[self.i][name]
            param.grad += self.lamda[self.i][name] * self.alpha[self.i]
            # param.grad += self.beta[self.i][self.k] * (u_state[name] - z_state[name])
            param.grad += self.beta[self.i][self.k] * \
                (u_state[name] - z_state[name]) * self.alpha[self.i]

    def _init_params(self, cfg, model_server):
        """Initialize parameters and hyperparameters."""
        # è·å–æœåŠ¡å™¨æ¨¡å‹çš„å‚æ•°çŠ¶æ€å­—å…¸
        model_state = model_server.state_dict()
        # å¤åˆ¶æœåŠ¡å™¨æ¨¡å‹å‚æ•°ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯çš„åˆå§‹æ¨¡å‹
        self.models = [copy.deepcopy(model_state) for _ in range(cfg.m)]
        # å°†è¦è¿”å›ç»™æœåŠ¡å™¨çš„ç»“æœself.send_back æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨å®¢æˆ·ç«¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„ç»“æœï¼Œè¿™äº›ç»“æœå°†è¢«å‘é€å›æœåŠ¡å™¨ã€‚å®ƒåŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š
        #
        # æœ¬åœ°æ¨¡å‹å‚æ•°ï¼šæ¯ä¸ªå®¢æˆ·ç«¯çš„å½“å‰æ¨¡å‹å‚æ•°ã€‚
        # æ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼šç”¨äºæ›´æ–°å’Œç»´æŠ¤çº¦æŸæ¡ä»¶çš„å¯¹å¶å˜é‡ã€‚
        # æƒ©ç½šå‚æ•°
        # ğ›½ ç”¨äºè°ƒæ•´æ¨¡å‹ä¸€è‡´æ€§çš„å‚æ•°ã€‚
        self.send_back = {}  # Results to send back to the server
        self.send_back["models"] = self.models  # local model parameters
        # to save local FGD/SGD steps# ä¿å­˜æœ¬åœ°FGD/SGDæ­¥æ•°
        self.steps = np.full((cfg.m, cfg.K + 1), 0)
        # local model accuracy# ä¿å­˜æœ¬åœ°æ¨¡å‹å‡†ç¡®ç‡
        self.accus = np.full((cfg.m, cfg.K + 1), 0, dtype=float)
        # é’ˆå¯¹ADMMç®—æ³•ï¼Œåˆå§‹åŒ–å¯¹å¶å˜é‡å’Œæƒ©ç½šå‚æ•°
        if self.alg in ["admm_insa", "admm_in", "admm"]:
            # dual variables lambda
            lamda = copy.deepcopy(model_state)  # åˆå§‹åŒ–æ‹‰æ ¼æœ—æ—¥ä¹˜å­ lambda
            for key in model_state.keys():
                lamda[key].zero_()  # å°†ä¹˜å­åˆå§‹åŒ–ä¸ºé›¶
            self.lamda = [copy.deepcopy(lamda)
                          for _ in range(cfg.m)]  # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯å¤åˆ¶æ‹‰æ ¼æœ—æ—¥ä¹˜å­
            self.send_back["lambda"] = self.lamda
            # penalty parameter beta # åˆå§‹åŒ–æƒ©ç½šå‚æ•° beta åœ¨å‡½æ•° _init_params ä¸­ï¼Œæƒ©ç½šå‚æ•°
            # ğ›½ è¢«åˆå§‹åŒ–ä¸ºé…ç½®æ–‡ä»¶ cfg ä¸­çš„æŒ‡å®šå€¼ cfg.betaã€‚
            self.beta = np.full((cfg.m, cfg.K + 2), cfg.beta,
                                dtype=float)  # round k from 1 to K
            self.send_back["beta"] = [cfg.beta] * cfg.m


# auxiliary functions
def init_optimizer(cfg, model, k):
    """Initialize the optimizer."""
    # è®¡ç®—å­¦ä¹ ç‡ï¼Œè€ƒè™‘å­¦ä¹ ç‡è¡°å‡
    # å¦‚æœè®¾ç½®äº†å­¦ä¹ ç‡è¡°å‡å› å­ lr_decayï¼Œåˆ™æ ¹æ®å½“å‰è®­ç»ƒè½®æ¬¡ k è®¡ç®—å­¦ä¹ ç‡
    lr = cfg.lr * np.power(cfg.lr_decay, k - 1) if cfg.lr_decay else cfg.lr
    # æ ¹æ®é…ç½®é€‰æ‹©ä¸åŒçš„ä¼˜åŒ–å™¨
    if cfg.optimizer == "sgd":
        # åˆå§‹åŒ– SGD ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨å­¦ä¹ ç‡ lrã€æƒé‡è¡°å‡ç³»æ•° decay å’ŒåŠ¨é‡ momentum
        optimizer = torch.optim.SGD(
            model.parameters(), lr=lr, weight_decay=cfg.decay, momentum=cfg.momentum
        )
    # åˆå§‹åŒ– Adam ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨å­¦ä¹ ç‡ lr
    elif cfg.optimizer == "adam":
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    # æœ€åè¿”å›åˆå§‹åŒ–åçš„ä¼˜åŒ–å™¨ï¼Œç”¨äºåç»­çš„æ¨¡å‹è®­ç»ƒæ­¥éª¤ã€‚
    return optimizer


def init_data_loader(cfg, dataset_i, shuffle=True):
    """Initialize the dataloader."""
    # å¦‚æœé…ç½®ä¸­è®¾ç½®äº†æ‰¹æ¬¡å¤§å° bsï¼Œåˆ™ä½¿ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ (SGD)
    if cfg.bs:  # SGD
        data_loader = DataLoader(dataset_i, batch_size=cfg.bs, shuffle=shuffle)
    else:  # FGD
        # ä½¿ç”¨ FGD (å…¨æ•°æ®æ¢¯åº¦ä¸‹é™)
        # å¦‚æœ bs ä¸º 0ï¼Œåˆ™ä½¿ç”¨å®Œæ•´æ•°æ®é›†ï¼Œå³å…¨æ¢¯åº¦ä¸‹é™ï¼ˆFGDï¼‰
        data_loader = DataLoader(
            dataset_i, batch_size=len(dataset_i), shuffle=shuffle)
    return data_loader


def l2_norm(x: nn.Module, y: nn.Module, square=False):
    """Calculate the l2 norm of x and y."""
    res = 0  # åˆå§‹åŒ– L2 èŒƒæ•°è®¡ç®—ç»“æœ
    # è®¡ç®— L2 èŒƒæ•°ï¼Œåœ¨ä¸è¿›è¡Œæ¢¯åº¦æ›´æ–°çš„æƒ…å†µä¸‹
    with torch.no_grad():
        # éå†ä¸¤ä¸ªæ¨¡å‹çš„å‚æ•°
        # without BN statistics
        for x_item, y_item in zip(x.parameters(), y.parameters()):
            # è®¡ç®—æ¯ä¸ªå‚æ•°ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶ç´¯ç§¯å¹³æ–¹å€¼
            res += torch.linalg.norm(x_item - y_item).square()
        # å¦‚æœ square ä¸º Falseï¼Œåˆ™è¿”å›å¹³æ–¹æ ¹ï¼ˆå³ L2 èŒƒæ•°ï¼‰
        res = res if square else torch.sqrt(res)
        # è¿”å› L2 èŒƒæ•°æˆ–å…¶å¹³æ–¹çš„å€¼
    return res.item()


def state2model(cfg: dict, state: dict):
    """Create a model with given parameters."""
    # åˆå§‹åŒ–æ¨¡å‹ï¼Œæ ¹æ®ç»™å®šé…ç½® cfg
    model = init_model(cfg, True)
    # åŠ è½½å‚æ•°å­—å…¸åˆ°æ¨¡å‹ä¸­ï¼Œå°† state ä¸­çš„å‚æ•°åŠ è½½åˆ°æ–°æ¨¡å‹ä¸­
    model.load_state_dict(state)
    # è¿”å›è¿™ä¸ªå…·æœ‰æŒ‡å®šå‚æ•°çš„æ¨¡å‹
    return model
